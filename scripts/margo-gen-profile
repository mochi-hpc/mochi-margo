#!/usr/bin/env python
# (C) 2015 The University of Chicago
#
# See COPYRIGHT in top-level directory.
# 

import matplotlib
matplotlib.use("Agg")
import base64
import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
from matplotlib.backends.backend_pdf import PdfPages
import operator
import sys
import os
import glob, re
from collections import defaultdict, OrderedDict

class ProfileGenerator:
	def __init__(self):
		self.name = "MargoProfileGenerator"

		self.cumulative = [dict(), dict()] #Separate dictionaries for origin and target in the form <KEY = RPC breadcrumb name, VALUE = cumulative time>
		self.count = dict() #Dictionary in the form <KEY = RPC breadcrumb name, VALUE = cumulative count>

		self.cumulativestat = [defaultdict(list), defaultdict(list)]  #Separate dictionaries for origin and target in the form <KEY = RPC breadcrumb name, VALUE = [list of individual times] >
		self.countstat = [defaultdict(list), defaultdict(list)] #Separate dictionaries for origin and target in the form <KEY = RPC breadcrumb name, VALUE = [list of individual counts] >

		self.poolsizehwm = defaultdict(list) #RPC handler pool size low water mark on the target, in the form <KEY = RPC breadcrumb name, VALUE = [list of individual low watermarks]>
		self.poolsizelwm = defaultdict(list) #RPC handler pool size high  water mark on the target, in the form <KEY = RPC breadcrumb name, VALUE = [list of individual low watermarks]>
		self.pooltotalsizehwm = defaultdict(list) #RPC handler pool total size low water mark on the target, in the form <KEY = RPC breadcrumb name, VALUE = [list of individual low watermarks]>
		self.pooltotalsizelwm = defaultdict(list) #RPC handler pool total size high  water mark on the target, in the form <KEY = RPC breadcrumb name, VALUE = [list of individual low watermarks]>

		self.countsparklines = defaultdict(list) #Sparkline data for RPC breadcrumb counts, in the form <KEY = <RPC breadcrumb name, sparkline_index>, VALUE = [list of individual counts for target instances]>
		self.timesparklines = [defaultdict(list), defaultdict(list)] #Sparkline data for RPC cumulative time, in the form <KEY = <RPC breadcrumb name, sparkline_index>, VALUE = [list of individual counts for origin or target instaces]>

		self.nodetomid = defaultdict(list) #List of margo instances on a given node
		self.midtoprovider = defaultdict(list) #List of registered provider types on a given margo instance
		self.provider_id = dict() #Used to generate a unique global ID for every provider. This is necessary in order to properly generate the graphViz file
		self.edgelistcount = dict() #Count information in the form <KEY = (source instance, source call, dest instance, destination call), VALYE = cumulative count >
		self.edgelisttime = dict() #Time information in the form <KEY = (source instance, source call, dest instance, destination call), VALYE = cumulative time >

		self.hash64tomid = dict() #Map a 64-bit hash to a margo instance address string
		self.rpc2name = dict() #Map the RPC ID to a proper registered name
		self.rpclist = list() #List of RPCs in the profile files
		self.onesidedrpclist = list() #List of one-sided RPCs
		self.pp = PdfPages('profile.pdf')

	# Takes in a list of hexadecimal RPC breadcrumbs and returns a list 
	# of breadcrumb names
	def __getrpc2name(self, o):
		output = []
		for i in o:
			l = list(i.split(' '))
			tmp = ""
			for j in (l[::-1])[1:]:
				if tmp != '':
					tmp = tmp+"->"+self.rpc2name.get(j, "UNKNOWN_RPC_ID")
				else:
					tmp = self.rpc2name.get(j, "UNKNOWN_RPC_ID")
			output.append((re.sub("(->)", "\\1 ", tmp, 0, re.DOTALL)).strip('\n'))
		return output

	# Boilerplate for graph generation
	# x_pos = positions on the x-axis
	# perf_arr = performance numbers to plot
	# is_stat_graph = whether this graph is a statistics graph or not
	# objects = list of stuff to plot on the x-axis
	def __gengraph(self, x_pos, perf_arr, xlabel, ylabel, title, txt=None, is_stat_graph=False, use_x_ticks=False, objects=None, labels=[None, None]):
		fig = plt.figure(num=None, figsize=(15, 10), dpi=80, facecolor='w', edgecolor='k')
		ax = fig.add_subplot(111)
		width=0.35

		if(is_stat_graph):
			ax.boxplot(perf_arr)
			ax.set_ylim(bottom=0)
		else:
			for i in range(0, len(perf_arr)):
				ax.bar(x_pos+i*width, perf_arr[i], width=width, align='center', alpha=0.5, label=labels[i])
				ax.legend(loc='best')
			
		if(use_x_ticks):
			ax.set_xticklabels(objects)
			if(not is_stat_graph):
				ax.set_xticks(x_pos+((width/2)*(len(perf_arr)-1)))
		else:
			plt.xticks([])

		ax.set_xlabel(xlabel, fontsize=16)
		ax.set_ylabel(ylabel, fontsize=16)
		ax.set_title(txt)
		fig.suptitle(title, fontsize=20, fontweight='bold')
		fig.savefig(self.pp, format='pdf')

		plt.close()
	

	# Boilerplate for sparkline graph generation
	# objects = list of stuff to plot on the x-axis
	# num_subplots = number of sparklines to draw
	# perf_arr = sparkline data to use for plotting
	def __gensparklinegraph(self, objects, title, num_subplots, perf_arr, txt=None):
		fig, ax = plt.subplots(num_subplots+1, figsize=(15,10))
		plt.subplots_adjust(hspace=0.3)
		fig.suptitle(title, fontsize=20, fontweight='bold')
		ax[0].text(0, 0.5, txt, style='italic',
			bbox={'facecolor': 'red', 'alpha': 0.5, 'pad': 10}, horizontalalignment='left')
		ax[0].set_xticks([])
		ax[0].set_yticks([])
		for k1, v1 in ax[0].spines.items():
			v1.set_visible(False)

		for i in range(0, num_subplots):
			v = perf_arr[i]
			ax[i+1].plot(v)

			for k1, v1 in ax[i+1].spines.items():
				v1.set_visible(False)

			ax[i+1].set_xticks([])
			ax[i+1].set_yticks([])
			ax[i+1].plot(len(v) - 1, v[len(v) - 1], 'r.')
			ax[i+1].fill_between(range(len(v)), v, len(v)*[min(v)], alpha=0.1)
			ax[i+1].set_title((objects[i]).strip('\n'), fontsize=8)

		plt.savefig(self.pp, format='pdf')
		plt.close()
		
		

	#Add to the edgelist that collects count information in the form <KEY = (source instance, source call, dest instance, destination call), VALYE = cumulative count >
	#Add to the edgelist that collects time information in the form <KEY = (source instance, source call, dest instance, destination call), VALUE = cumulative time >
	def __addtoedgelist(self, name, mid, addr_hash, cumulative, count):
		raw_rpc_calls = list(name.split(" "))
		del raw_rpc_calls[len(raw_rpc_calls) - 1]

		if(len(raw_rpc_calls) > 1):
			self.edgelistcount[(mid, raw_rpc_calls[1], addr_hash, raw_rpc_calls[0])] = self.edgelistcount.get((mid, raw_rpc_calls[1], addr_hash, raw_rpc_calls[0]), 0) + int(count)
			self.edgelisttime[(mid, raw_rpc_calls[1], addr_hash, raw_rpc_calls[0])] = self.edgelisttime.get((mid, raw_rpc_calls[1], addr_hash, raw_rpc_calls[0]), 0) + float(cumulative)
		else:
			self.edgelistcount[(mid, raw_rpc_calls[0], addr_hash, raw_rpc_calls[0])] = self.edgelistcount.get((mid, raw_rpc_calls[0], addr_hash, raw_rpc_calls[0]), 0) + int(count)
			self.edgelisttime[(mid, raw_rpc_calls[0], addr_hash, raw_rpc_calls[0])] = self.edgelisttime.get((mid, raw_rpc_calls[0], addr_hash, raw_rpc_calls[0]), 0) + float(cumulative)
			

	#Generate the nodes of the graph
	#Grey: Nodes
	#Blue: Margo instances
	#White: Providers
	def __gengraphnodes(self):
		node_string = "/* First describe and generate the nodes and entities in the graph */\n"
		for (node, midlist) in self.nodetomid.items():
			node_string += ("subgraph cluster" + node + " {\n")
			node_string += ("node [style=filled, color=white];\n style=filled;\n color=lightgrey;\n")
			for mid in midlist:
				node_string += ("subgraph cluster" + mid.replace("+", "_").replace(":", "_").replace(".","_").replace("//", "_").replace("/", "_") + " {\n")
				node_string += ("node [style=filled];\n color=lightblue;\n")
				for (provider_type, provider_id) in self.midtoprovider[mid]:
					node_string += (str(provider_id) + ";\n")
				node_string += ("label = \"" + mid + "\";\n")
				node_string += ("}\n")
			node_string += ("label = \"" + node + "\";\n")
			node_string += ("}\n")
		
		return node_string

	#Generate weighted edges of the form (cumulative time, cumulative count)
	#(source_provider_id (unique), source_call, destination_provider_id (globally unique), destination_call)
	#IMPT: We derive the "provider type" from the RPC call by assuming that every RPC call has the form <provider_type>_<RPC_call>
	#For instance, the call sdskv_get_keys has the provider type as "sdskv" and RPC call as "get_keys"
	def __gengraphedges(self):
		edge_string = "/* Generate a list of weighted edges for the graph in the form (count, time) */\n"
		for edge_info, count in self.edgelistcount.items():
			cumulative_time = self.edgelisttime[edge_info]
			source_provider_id = ""
			dest_provider_id = ""

			#IMPT: We only have source and destination margo instances. We need to derive the id's for the providers inside the margo instance that made
			#the call.
			#Assumption: We make an assumption that there is only one provider of a given type inside a margo instance. If there are multiple, then
			#the edge represents the cumulative counts or time values for all providers of a given type.
			(source, source_call, destination, dest_call) = edge_info
			destination = self.hash64tomid.get(destination, "UNKNOWN_TARGET")
			source_provider = ((self.rpc2name.get(str(source_call), "UNKNOWN")).split("_"))[0]
			dest_provider = ((self.rpc2name.get(str(dest_call), "UNKNOWN")).split("_"))[0]

			if source_provider != "UNKNOWN" and dest_provider != "UNKNOWN":

				for (provider_type, provider_id) in self.midtoprovider[source]:
					if source_provider == provider_type:
						source_provider_id = provider_id
			
				for (provider_type, provider_id) in self.midtoprovider[destination]:
					if dest_provider == provider_type:
						dest_provider_id = provider_id
				dest_call = self.rpc2name[str(dest_call)]
			
				edge_string += (str(source_provider_id) + " -> " + str(dest_provider_id) + "[label=\""+dest_call+","+str(count)+","+str(cumulative_time)+"\", weight=\""+str(dest_call)+","+str(count)+","+str(cumulative_time)+"\"];\n")

		return edge_string			


	# Remove profile information for one-sided RPCs that are characterized by having an origin entry but no corresponding target entry
	def __filteroutonesidedrpcs(self):
		for name in self.rpclist:
			if name in self.cumulative[0].keys() and name not in self.cumulative[1].keys():
				del self.cumulative[0][name]
				del self.cumulativestat[0][name]
				del self.count[name]
				del self.countstat[0][name]
				self.onesidedrpclist.append(name)

	# Read the current working directory for profile*.csv files and populate the relevant data-structures	
	# Profile files are expected to be in the following format:
	#   N = num RPC's registered on this instance
	#   Followed by N lines of <RPC ID>,<RPC NAME>
	#   3 lines for Margo internal routines: trigger elapsed, progress_elapsed_zero_timeout, progress_elapsed_nonzero_timeout
	#   Followed by actual breadcrumb data in the form <name, avg, rpc_breadcrumb, addr_hash, origin_or_target, cumulative, _min, _max, count,  handler_max, handler_min, handler_cumulative>
	def readfiles(self):
		files = glob.glob(str(os.getcwd())+"/*.csv") #Read all *.csv files in CURRENT_WORKING_DIRECTORY
		for f in files:
			f1 = open(f, "r")
			contents = f1.readlines()
			num_registered_rpcs = int(contents[0]) #First line is always number of RPC's registered with the margo instance generating this particular profile file

			self_hash, mid = ((str(contents[1])).strip("\n")).split(',') #Second line is always the margo instance network name, succeeding the hash of this name
			self.hash64tomid[int(self_hash)] = mid
			node = (((os.path.basename(f1.name)).split('-')[1]).replace(".", ""))
			self.nodetomid[node].append(mid) #Collect list of margo instances on this node

			if num_registered_rpcs > 0:
				providers = set()
				for lines in contents[2:2+num_registered_rpcs]: #Populate map of RPC ID's to RPC names
					k, v = lines.split(',', 2)
					self.rpc2name[k] = v
					provider_type = v.split("_")[0] #IMPT: Assumption made: Every RPC registered has the form <provider_type>_RPC_type
					providers.add(provider_type)

				for provider in providers:
					self.midtoprovider[mid].append((provider, provider + "_" + str(self.provider_id.get(provider, 0))))
					self.provider_id[provider] = self.provider_id.get(provider, 0) + 1 #Increment counter for the provider_type
		
			contents_ = contents[2 + num_registered_rpcs:] 
			for i in range(0,len(contents_),2):
				breadcrumb_line = contents_[i]
				spark_line = contents_[i+1]
				#Even lines contain breadcrumb data
				name, avg, rpc_breadcrumb, \
					addr_hash, origin_or_target, \
						cumulative, _min, _max, count, \
							abt_pool_size_hwm, abt_pool_size_lwm, abt_pool_size_cumulative,  \
								abt_pool_total_size_hwm, abt_pool_total_size_lwm, abt_pool_total_size_cumulative = breadcrumb_line.split(',', 15)

				self.rpclist.append(str(name))
				origin_or_target = int(origin_or_target)
				addr_hash = int(addr_hash)

				self.count[name] = self.count.get(name, 0) + int(count)
				self.cumulative[origin_or_target][name] = self.cumulative[origin_or_target].get(name, 0.0) + float(cumulative)
				self.countstat[origin_or_target][name].append(int(count))
				self.cumulativestat[origin_or_target][name].append(float(cumulative))

				#Add Argobot pool size information if this is a target-side data point
				if(origin_or_target == 1):
					if (float(abt_pool_size_lwm) >= 0): self.poolsizelwm[name].append(float(abt_pool_size_lwm))
					if(float(abt_pool_size_hwm) < 100000): self.poolsizehwm[name].append(float(abt_pool_size_hwm))
					if (float(abt_pool_total_size_lwm) >= 0): self.pooltotalsizelwm[name].append(float(abt_pool_total_size_lwm))
					if(float(abt_pool_total_size_hwm) < 100000): self.pooltotalsizehwm[name].append(float(abt_pool_total_size_hwm))

				#If this is an origin-side data-point, collect information about the breadcrumb in order to generate a topology graph
				else:
					self.__addtoedgelist(name, mid, addr_hash, cumulative, count)

				#Oddlines contain sparkline data
				value_list = spark_line.split(";")
				name, origin_or_target = (value_list[0]).split(',')
				origin_or_target = int(origin_or_target)
				for v in value_list[1:]:
					if v != "\n":
						t, c, i = v.split(',')
						if(origin_or_target):
							self.countsparklines[(name, int(i))].append(float(c))
						self.timesparklines[origin_or_target][(name, int(i))].append(float(t))
			
			f1.close()

		# Filter out one sided RPCs. For now, we are handling this in post-processing and issuing a warning to the user
		# when we detect that the profiles contain one-sided RPCs
		self.__filteroutonesidedrpcs()

	#Cumulative counts of RPC breadcrumbs across the entire profile
	#Sort and display only top 5
	def gencountgraph(self):
		self.count = OrderedDict(sorted(self.count.items(), key=operator.itemgetter(1), reverse=True)[:5])
		objects = self.__getrpc2name(self.count.keys())
		txt = 'Sort breadcrumbs by cumulative call count\n Display top-5 breadcrumbs in descending order of call count'
		x_pos = np.arange(len(objects))
		perf = self.count.values()
		perf = [x/2 for x in perf] #Invocation of an RPC on the origin and execution of the RPC on the target counts as 1, not 2
		self.__gengraph(x_pos, [perf], 'Breadcrumb ID', 'Count', 
			'Breadcrumb Call Counts', txt=txt, is_stat_graph=False, use_x_ticks=True, objects=objects)

	#Display statistics on origin and target for the top 5 RPC breadcrumbs determined by cumulative count 
	def gencountstatgraph(self):
		graph_names = ['Statistics of Breadcrumb Call Counts: Origin', 'Statistics of Breadcrumb Call Counts: Target']
		txt = ['Display statistics of distribution of individual, cumulative call-counts for the top-5 breadcrumbs, across all origin instances', \
			'Display statistics of distribution of individual, cumulative call-counts for the top-5 breadcrumbs, across all target instances']
		for i in 0,1:
			x = defaultdict(list)
			for k,v in self.countstat[i].items():
				if k in self.count:
					x[k] = v

			x = OrderedDict(sorted(x.items(), key=lambda it: sum(it[1]), reverse=True))
				
			perf_arr = []

			for k in x:
				perf_arr.append(x[k])

			objects = self.__getrpc2name(x.keys())
			x_pos = np.arange(len(objects))
			self.__gengraph(x_pos, perf_arr, 'Breadcrumb ID', 
				'Count', graph_names[i], txt=txt[i], is_stat_graph=True, use_x_ticks=True, objects=objects)

	#Display raw distribution of counts on the target for the breadcrumb with the highest cumulative count
	def gencountrawgraph(self):
		x = defaultdict(list)
		txt = 'Display raw distribution of cumulative call-counts across all target instances for the breadcrumb with highest cumulative call-count'
		for k, v in self.countstat[1].items():
			if k in self.count:
				x[k] = v

		y = sorted(x.items(), key=lambda it: sum(it[1]), reverse=True)[:1]
		[(a,b)] = y
	
		perf = b
 
		x_pos = np.arange(len(perf))
		self.__gengraph(x_pos, [perf], 'Breadcrumb ID: ' + self.__getrpc2name([a])[0], 
			'Count', 'Raw Breadcrumb Call Counts: Target', txt=txt, is_stat_graph=False, use_x_ticks=False)

	# Cumulative time for breadcrumbs, both on client as well as provider
	# Sort and display only top 5
	# 0 = origin, 1 = target
	def gencumulativegraph(self):
		graph_name = 'Cumulative Time'
		labels = ['Origin', 'Target']
		perf_arr = []
		txt = 'Sort breadcrumbs by cumulative time on the origin \n Display top-5 breadcrumbs in descending order of cumulative time on origin and target'

		#Sort by top-5 side origin-side breadcrumbs by cumulative time
		keys = (OrderedDict(sorted(self.cumulative[0].items(), key=operator.itemgetter(1), reverse=True)[:5])).keys()
		objects = self.__getrpc2name(keys)
		x_pos = np.arange(len(objects))
		
		for i in 0,1:
			perf = []
			for k in keys:
				perf.append(self.cumulative[i][k])

			perf_arr.append(perf)

		self.__gengraph(x_pos, perf_arr, 'Breadcrumb ID', 'Seconds',
		graph_name, txt=txt, is_stat_graph=False, use_x_ticks=True, objects=objects, labels=labels)

	#Display statistics on origin and target for the top 5 RPC breadcrumbs determined by cumulative time
	# 0 = origin, 1 = target
	def gencumulativestatgraph(self):
		graph_names = ['Statistics of Cumulative Time: Origin', 'Statistics of Cumulative Time: Target']
		txt = ['Display statistics of distribution of individual, cumulative time for the top-5 breadcrumbs, across all origin instances', \
			'Display statistics of distribution of individual, cumulative time for the top-5 breadcrumbs, across all target instances']

		keys = (OrderedDict(sorted(self.cumulative[0].items(), key=operator.itemgetter(1), reverse=True)[:5])).keys()
		objects = self.__getrpc2name(keys)
		x_pos = np.arange(len(objects))

		for i in 0,1:
			x = defaultdict(list)
			for k,v in self.cumulativestat[i].items():
				if k in keys:
						x[k] = v

			x = OrderedDict(sorted(x.items(), key=lambda it: sum(it[1]), reverse=True))

			perf_arr = []

			for k in x:
				perf_arr.append(x[k])

			self.__gengraph(x_pos, perf_arr, 'Breadcrumb ID',
				'Seconds', graph_names[i], txt=txt[i], is_stat_graph=True, use_x_ticks=True, objects=objects)

	#Display raw distribution of times on the origin and target for the breadcrumb with the highest cumulative time
	# 0 = origin, 1 = target
	def gencumulativerawgraph(self):
		graph_names = ['Raw Cumulative Time: Origin', 'Raw Cumulative Time: Target']
		keys = (OrderedDict(sorted(self.cumulative[0].items(), key=operator.itemgetter(1), reverse=True)[:5])).keys()
		txt = ['Display raw distribution of cumulative times across all origin instances for the breadcrumb with highest cumulative time', \
			'Display raw distribution of cumulative time across all target instances for the breadcrumb with highest cumulative time'] 
		
		for i in 0,1:
			x = defaultdict(list)
			for k,v in self.cumulativestat[i].items():
				if k in keys:
					x[k] = v

			y = sorted(x.items(), key=lambda it: sum(it[1]), reverse=True)[:1]
			[(a,b)] = y

			perf = b
			x_pos = np.arange(len(perf))
			self.__gengraph(x_pos, [perf], 'Breadcrumb ID: ' + self.__getrpc2name([a])[0],
				'Seconds', graph_names[i], txt=txt[i], is_stat_graph=False, use_x_ticks=False)

	#Display statistics of the low and highwater mark of RPC handler pool sizes across target instances
	def genargobotpoolsizegraph(self):
		graph_names = ['Statistics of Low-water-mark of Argobot Pool Size: Target', 'Statistics of High-water-mark of Argobot Pool Size: Target']
		lwm = OrderedDict(defaultdict(list))
		hwm = OrderedDict(defaultdict(list))
		keys = (OrderedDict(sorted(self.cumulative[0].items(), key=operator.itemgetter(1), reverse=True)[:5])).keys()
		txt = ['Sort breadcrumbs by cumulative time on the origin \n Display the distribution of low-water mark for the Argobot pool size across target instances for the top-5 breadcrumbs sorted by cumulative time \
\n Argobot pool sizes are captured on the target when an RPC response is sent back to origin' \
			,'Sort breadcrumbs by cumulative time on the origin \n Display the distribution of high-water mark for the Argobot pool size across target instances for the top-5 breadcrumbs sorted by cumulative time \
\n Argobot pool sizes are captured on the target when an RPC response is sent back to origin']

		for k in keys:
			lwm[k] = self.poolsizelwm[k]
			hwm[k] = self.poolsizehwm[k]
		
		l = [lwm, hwm]

		for i in 0,1:
			perf_arr = []
			for k in l[i]:
				perf_arr.append(l[i][k])

			objects = self.__getrpc2name(l[i].keys())
			x_pos = np.arange(len(objects))

			self.__gengraph(x_pos, perf_arr, 'Breadcrumb ID',
				'Pool Size', graph_names[i], txt=txt[i], is_stat_graph=True, use_x_ticks=True, objects=objects)

	#Display statistics of the low and highwater mark of RPC handler total pool sizes across target instances
	def genargobottotalpoolsizegraph(self):
		graph_names = ['Statistics of Low-water-mark of Argobot Total Pool Size: Target', 'Statistics of High-water-mark of Argobot Total Pool Size: Target']
		lwm = OrderedDict(defaultdict(list))
		hwm = OrderedDict(defaultdict(list))
		keys = (OrderedDict(sorted(self.cumulative[0].items(), key=operator.itemgetter(1), reverse=True)[:5])).keys()
		txt = ['Sort breadcrumbs by cumulative time on the origin \n Display the distribution of low-water mark for the Argobot total pool size across target instances for the top-5 breadcrumbs sorted by cumulative time \
\n Argobot pool sizes are captured on the target when an RPC response is sent back to origin \n \
Argobot total pool size includes the number of suspended work items in addition to the number of runnable work items'
			,'Sort breadcrumbs by cumulative time on the origin \n Display the distribution of high-water mark for the Argobot total pool size across target instances for the top-5 breadcrumbs sorted by cumulative time \
\n Argobot pool sizes are captured on the target when an RPC response is sent back to origin \n \
Argobot total pool size includes the number of suspended work items in addition to the number of runnable work items']

		for k in keys:
			lwm[k] = self.pooltotalsizelwm[k]
			hwm[k] = self.pooltotalsizehwm[k]
		
		l = [lwm, hwm]

		for i in 0,1:
			perf_arr = []
			for k in l[i]:
				perf_arr.append(l[i][k])

			objects = self.__getrpc2name(l[i].keys())
			x_pos = np.arange(len(objects))

			self.__gengraph(x_pos, perf_arr, 'Breadcrumb ID',
				'Total Pool Size', graph_names[i], txt=txt[i], is_stat_graph=True, use_x_ticks=True, objects=objects)

	#Generate and display sparklines for the top-5 RPC breadcrumbs, sorted by cumulative counts
	def gencountsparklines(self):
		x = defaultdict(list)
		txt = 'Sort breadcrumbs by cumulative call-count \n \
For the top-5 breadcrumbs, display sparklines for the cumulative call-count across all target instances'
		for (k, i), v in self.countsparklines.items():
			if k in self.count:
				x[k].append(sum(v))

		x = OrderedDict(sorted(x.items(), key=lambda it: sum(it[1]), reverse=True))
		perf_arr = []
		for k, v in x.items():
			perf_arr.append(v)

		objects = self.__getrpc2name(x.keys())
		if(len(x.items()) > 0):
			self.__gensparklinegraph(objects=objects, title='Sparkline for cumulative count on target', num_subplots=len(x.items()), perf_arr=perf_arr, txt=txt)

	#Generate and display sparklines for the top-5 RPC breadcrumbs, sorted by cumulative time, on both origin and target
	# 0 = origin, 1 = target
	def gentimesparklines(self):
		graph_names = ['Sparkline for cumulative time on origin', 'Sparkline for cumulative time on target']
		keys = (OrderedDict(sorted(self.cumulative[0].items(), key=operator.itemgetter(1), reverse=True)[:5])).keys()
		txt = ['Sort breadcrumbs by cumulative call-time on origin \n \
For the top-5 breadcrumbs, display sparklines for the cumulative call-time across all origin instances', \
			'Sort breadcrumbs by cumulative call-time on origin \n \
For the top-5 breadcrumbs, display sparklines for the cumulative call-time across all target instances']

		for p in 0,1:
			x = defaultdict(list)
			for (k, i), v in self.timesparklines[p].items():
				if k in keys:
					x[k].append(sum(v))

			x = OrderedDict(sorted(x.items(), key=lambda it: sum(it[1]), reverse=True))
			objects = self.__getrpc2name(x.keys())
			perf_arr = []

			for k, v in x.items():
				perf_arr.append(v)

			
			if(len(x.items()) > 0):
				self.__gensparklinegraph(objects=objects, title=graph_names[p], num_subplots=len(x.items()), perf_arr=perf_arr, txt=txt[p])


	#Generate a graphViz file that describes the structure of the RPC communication graph in the DOT language
	#From this file, generate a PNG image using: $dot -Tpng graph.gv -o graph.png
	def gengvfile(self):
		f = open("graph.gv", "w")
		f.write("digraph G {\n")

		f.write(self.__gengraphnodes())
		f.write(self.__gengraphedges())

		f.write("}\n")
		f.close()

	# Generate and display warnings about missing profile data, etc
	def genwarnings(self):
		if (len(self.onesidedrpclist) != 0):
			txt = ""
			rpcnamelist = self.__getrpc2name(self.onesidedrpclist)

			for index, rpcname in enumerate(rpcnamelist):
				txt = txt + str(index + 1) + ". " + rpcname + "\n"

			txt = "Displaying profile information for one-sided RPCs is not yet supported. The following one-sided \
RPCs were detected in the profile files: \n\n" + txt

			fig, ax = plt.subplots(1, figsize=(15,4))
			plt.subplots_adjust(hspace=0.5)
			fig.suptitle("Profile Generator Warnings", fontsize=20, fontweight='bold')
			ax.text(0, 0.5, txt, style='italic',
				bbox={'facecolor': 'yellow', 'alpha': 0.5, 'pad': 10}, horizontalalignment='left')
			ax.set_xticks([])
			ax.set_yticks([])
			for k1, v1 in ax.spines.items():
				v1.set_visible(False)

			plt.savefig(self.pp, format='pdf')
			plt.close()
			
	def finalize(self):
		self.pp.close()
	
def main():
	print
	print
	print ("*******************MARGO Profile Generator******************")
	print
	print ("Reading CSV files from: " + os.getcwd())

	p = ProfileGenerator()
	p.readfiles()

	p.gencountgraph()
	p.gencountstatgraph()
	p.gencountrawgraph()

	p.gencumulativegraph()
	p.gencumulativestatgraph()
	p.gencumulativerawgraph()

	p.genargobotpoolsizegraph()
	p.genargobottotalpoolsizegraph()

	p.gencountsparklines()
	p.gentimesparklines()

	p.gengvfile()

	p.genwarnings()

	p.finalize()

	print ("Done.")
	print
	print ("************************************************************")


main()
