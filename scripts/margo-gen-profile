#!/usr/bin/env python

import matplotlib
matplotlib.use("Agg")
import base64
import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
from matplotlib.backends.backend_pdf import PdfPages
import operator
import sys
import os
import glob, re
from collections import defaultdict, OrderedDict

class ProfileGenerator:
	def __init__(self):
		self.name = "MargoProfileGenerator"

		self.cumulative = [dict(), dict()] #Separate dictionaries for origin and target in the form <KEY = RPC breadcrumb name, VALUE = cumulative time>
		self.count = dict() #Dictionary in the form <KEY = RPC breadcrumb name, VALUE = cumulative count>

		self.cumulativestat = [defaultdict(list), defaultdict(list)]  #Separate dictionaries for origin and target in the form <KEY = RPC breadcrumb name, VALUE = [list of individual times] >
		self.countstat = [defaultdict(list), defaultdict(list)] #Separate dictionaries for origin and target in the form <KEY = RPC breadcrumb name, VALUE = [list of individual counts] >

		self.poolsizehwm = defaultdict(list) #RPC handler pool size low water mark on the target, in the form <KEY = RPC breadcrumb name, VALUE = [list of individual low watermarks]>
		self.poolsizelwm = defaultdict(list) #RPC handler pool size high  water mark on the target, in the form <KEY = RPC breadcrumb name, VALUE = [list of individual low watermarks]>

		self.countsparklines = defaultdict(list)
		self.timesparklines = [defaultdict(list), defaultdict(list)]

		self.rpc2name = dict() #Map the RPC ID to a proper registered name
		self.pp = PdfPages('profile.pdf')

	# Takes in a list of hexadecimal RPC breadcrumbs and returns a list 
	# of breadcrumb names
	def __getrpc2name(self, o):
		output = []
		for i in o:
			l = list(i.split(' '))
			tmp = ""
			for j in (l[::-1])[1:]:
				if tmp != '':
					tmp = tmp+"->"+self.rpc2name.get(j, "UNKNOWN_RPC_ID")
				else:
					tmp = self.rpc2name.get(j, "UNKNOWN_RPC_ID")
			output.append((re.sub("(->)", "\\1 ", tmp, 0, re.DOTALL)).strip('\n'))
		return output

	# Boilerplate for graph generation
	# x_pos = positions on the x-axis
	# perf_arr = performance numbers to plot
	# is_stat_graph = whether this graph is a statistics graph or not
	# objects = list of stuff to plot on the x-axis
	def __gengraph(self, x_pos, perf_arr, xlabel, ylabel, title, is_stat_graph=False, use_x_ticks=False, objects=None, labels=[None, None]):
		fig = plt.figure(num=None, figsize=(15, 10), dpi=80, facecolor='w', edgecolor='k')
		ax = fig.add_subplot(111)
		width=0.35

		if(is_stat_graph):
			ax.boxplot(perf_arr)
			ax.set_ylim(ymin=0)
		else:
			for i in range(0, len(perf_arr)):
				ax.bar(x_pos+i*width, perf_arr[i], width=width, align='center', alpha=0.5, label=labels[i])
				ax.legend(loc='best')
			
		if(use_x_ticks):
			ax.set_xticklabels(objects)
			if(not is_stat_graph):
				ax.set_xticks(x_pos+((width/2)*(len(perf_arr)-1)))
		else:
			plt.xticks([])

		ax.set_xlabel(xlabel)
		ax.set_ylabel(ylabel)
		ax.set_title(title, fontsize=16)
		fig.savefig(self.pp, format='pdf')

		plt.close()
	

	def __gensparklinegraph(self, objects, title, num_subplots, perf_arr):
		fig, ax = plt.subplots(num_subplots, figsize=(15,10))
		plt.subplots_adjust(hspace=0.3)
		fig.suptitle(title, fontsize=16)
		for i in range(0, num_subplots):
			v = perf_arr[i]
			ax[i].plot(v)

			for k1, v1 in ax[i].spines.items():
				v1.set_visible(False)

			ax[i].set_xticks([])
			ax[i].set_yticks([])
			ax[i].plot(len(v) - 1, v[len(v) - 1], 'r.')
			ax[i].fill_between(range(len(v)), v, len(v)*[min(v)], alpha=0.1)
			ax[i].set_title((objects[i]).strip('\n'), fontsize=8)

		plt.savefig(self.pp, format='pdf')
		plt.close()
		
		
	
	# Read the current working directory for profile*.csv files and populate the relevant data-structures	
	# Profile files are expected to be in the following format:
	#   N = num RPC's registered on this instance
	#   Followed by N lines of <RPC ID>,<RPC NAME>
	#   3 lines for Margo internal routines: trigger elapsed, progress_elapsed_zero_timeout, progress_elapsed_nonzero_timeout
	#   Followed by actual breadcrumb data in the form <name, avg, rpc_breadcrumb, addr_hash, origin_or_target, cumulative, _min, _max, count,  handler_max, handler_min, handler_cumulative>
	def readfiles(self):
		files = glob.glob(str(os.getcwd())+"/*.csv") #Read all *.csv files in CURRENT_WORKING_DIRECTORY
		for f in files:
			f1 = open(f, "r")
			contents = f1.readlines()
			num_registered_rpcs = int(contents[0]) #First line is always number of RPC's registered with the margo instance generating this particular profile file
			if num_registered_rpcs > 0:
				for lines in contents[1:num_registered_rpcs]: #Populate map of RPC ID's to RPC names
					k, v = lines.split(',', 2)
					self.rpc2name[k] = v
			contents_ = contents[1 + 6 + num_registered_rpcs:] #Skip 3 internal Margo routines (progress w/o zero timeout, trigger_elapsed) and corresponding sparklines, which are useless
			for i in range(0,len(contents_),2):
				breadcrumb_line = contents_[i]
				spark_line = contents_[i+1]
				#Even lines contain breadcrumb data
				name, avg, rpc_breadcrumb, \
					addr_hash, origin_or_target, \
						cumulative, _min, _max, count, \
							handler_max, handler_min, handler_cumulative = breadcrumb_line.split(',', 12)
				origin_or_target = int(origin_or_target)
				addr_hash = int(addr_hash)

				self.count[name] = self.count.get(name, 0) + int(count)
				self.cumulative[origin_or_target][name] = self.cumulative[origin_or_target].get(name, 0.0) + float(cumulative)
				self.countstat[origin_or_target][name].append(int(count))
				self.cumulativestat[origin_or_target][name].append(float(cumulative))

				if(origin_or_target == 1):
					if (float(handler_min) >= 0): self.poolsizelwm[name].append(float(handler_min))
					if(float(handler_max) < 100000): self.poolsizehwm[name].append(float(handler_max))

				#Oddlines contain sparkline data
				value_list = spark_line.split(";")
				name, origin_or_target = (value_list[0]).split(',')
				origin_or_target = int(origin_or_target)
				for v in value_list[1:]:
					if v != "\n":
						t, c, i = v.split(',')
						if(origin_or_target):
							self.countsparklines[(name, int(i))].append(float(c))
						self.timesparklines[origin_or_target][(name, int(i))].append(float(t))
			
			f1.close()

	#Cumulative counts of RPC breadcrumbs across the entire profile
	#Sort and display only top 5
	def gencountgraph(self):
		self.count = OrderedDict(sorted(self.count.items(), key=operator.itemgetter(1), reverse=True)[:5])
		objects = self.__getrpc2name(self.count.keys())
		x_pos = np.arange(len(objects))
		perf = self.count.values()
		perf = [x/2 for x in perf] #Invocation of an RPC on the origin and execution of the RPC on the target counts as 1, not 2
		self.__gengraph(x_pos, [perf], 'Breadcrumb ID', 'Count', 
			'Breadcrumb Call Counts', is_stat_graph=False, use_x_ticks=True, objects=objects)

	#Display statistics on origin and target for the top 5 RPC breadcrumbs determined by cumulative count 
	def gencountstatgraph(self):
		graph_names = ['Statistics of Breadcrumb Call Counts: Origin', 'Statistics of Breadcrumb Call Counts: Target']
		for i in 0,1:
			x = defaultdict(list)
			for k,v in self.countstat[i].items():
				if k in self.count:
					x[k] = v

			x = OrderedDict(sorted(x.items(), key=lambda it: sum(it[1]), reverse=True))
				
			perf_arr = []

			for k in x:
				perf_arr.append(x[k])

			objects = self.__getrpc2name(x.keys())
			x_pos = np.arange(len(objects))
			self.__gengraph(x_pos, perf_arr, 'Breadcrumb ID', 
				'Count', graph_names[i], is_stat_graph=True, use_x_ticks=True, objects=objects)

	#Display raw distribution of counts on the target for the breadcrumb with the highest cumulative count
	def gencountrawgraph(self):
		x = defaultdict(list)
		for k, v in self.countstat[1].items():
			if k in self.count:
				x[k] = v

		y = sorted(x.items(), key=lambda it: sum(it[1]), reverse=True)[:1]
		[(a,b)] = y
	
		perf = b
 
		x_pos = np.arange(len(perf))
		self.__gengraph(x_pos, [perf], 'Breadcrumb ID: ' + self.__getrpc2name([a])[0], 
			'Count', 'Raw Breadcrumb Call Counts: Target', is_stat_graph=False, use_x_ticks=False)

	# Cumulative time for breadcrumbs, both on client as well as provider
	# Sort and display only top 5
	def gencumulativegraph(self):
		graph_name = 'Cumulative Time'
		labels = ['Origin', 'Target']
		perf_arr = []

		#Sort by top-5 side origin-side breadcrumbs by cumulative time
		keys = (OrderedDict(sorted(self.cumulative[0].items(), key=operator.itemgetter(1), reverse=True)[:5])).keys()
		objects = self.__getrpc2name(keys)
		x_pos = np.arange(len(objects))
		
		for i in 0,1:
			perf = []
			for k in keys:
				perf.append(self.cumulative[i][k])

			perf_arr.append(perf)

	       	self.__gengraph(x_pos, perf_arr, 'Breadcrumb ID', 'Seconds',
	        	graph_name, is_stat_graph=False, use_x_ticks=True, objects=objects, labels=labels)

	#Display statistics on origin and target for the top 5 RPC breadcrumbs determined by cumulative time
	def gencumulativestatgraph(self):
		graph_names = ['Statistics of Cumulative Time: Origin', 'Statistics of Cumulative Time: Target']

		keys = (OrderedDict(sorted(self.cumulative[0].items(), key=operator.itemgetter(1), reverse=True)[:5])).keys()
		objects = self.__getrpc2name(keys)
                x_pos = np.arange(len(objects))

		for i in 0,1:
	                x = defaultdict(list)
        	        for k,v in self.cumulativestat[i].items():
                	        if k in keys:
                        	        x[k] = v

	                x = OrderedDict(sorted(x.items(), key=lambda it: sum(it[1]), reverse=True))

			perf_arr = []

        	        for k in x:
				perf_arr.append(x[k])

	                self.__gengraph(x_pos, perf_arr, 'Breadcrumb ID',
        	                'Seconds', graph_names[i], is_stat_graph=True, use_x_ticks=True, objects=objects)

	#Display raw distribution of times on the origin and target for the breadcrumb with the highest cumulative time
	def gencumulativerawgraph(self):
		graph_names = ['Raw Cumulative Time: Origin', 'Raw Cumulative Time: Target']
		keys = (OrderedDict(sorted(self.cumulative[0].items(), key=operator.itemgetter(1), reverse=True)[:5])).keys()
		
		for i in 0,1:
	                x = defaultdict(list)
        	        for k,v in self.cumulativestat[i].items():
                	        if k in keys:
                        	        x[k] = v

	                y = sorted(x.items(), key=lambda it: sum(it[1]), reverse=True)[:1]
        	        [(a,b)] = y

                	perf = b
	                x_pos = np.arange(len(perf))
        	        self.__gengraph(x_pos, [perf], 'Breadcrumb ID: ' + self.__getrpc2name([a])[0],
                	        'Seconds', graph_names[i], is_stat_graph=False, use_x_ticks=False)

	#Display statistics of the low and highwater mark of RPC handler pool sizes across target instances
	def genargobotpoolsizegraph(self):
		graph_names = ['Statistics of Low-water-mark of Argobot Pool Size: Target', 'Statistics of High-water-mark of Argobot Pool Size: Target']
		lwm = OrderedDict(defaultdict(list))
		hwm = OrderedDict(defaultdict(list))
		keys = (OrderedDict(sorted(self.cumulative[0].items(), key=operator.itemgetter(1), reverse=True)[:5])).keys()

		for k in keys:
			lwm[k] = self.poolsizelwm[k]
			hwm[k] = self.poolsizehwm[k]
		
		l = [lwm, hwm]

		for i in 0,1:
			perf_arr = []
	                for k in l[i]:
        	                perf_arr.append(l[i][k])

	                objects = self.__getrpc2name(l[i].keys())
        	        x_pos = np.arange(len(objects))

                	self.__gengraph(x_pos, perf_arr, 'Breadcrumb ID',
                        	'Pool Size', graph_names[i], is_stat_graph=True, use_x_ticks=True, objects=objects)

	def gencountsparklines(self):
		x = defaultdict(list)
		for (k, i), v in self.countsparklines.items():
			if k in self.count:
				x[k].append(sum(v))

		x = OrderedDict(sorted(x.items(), key=lambda it: sum(it[1]), reverse=True))
		perf_arr = []

		for i in range(0, len(x.items())):
			k, v = (x.items())[i]
			perf_arr.append(v)

		objects = self.__getrpc2name(x.keys())
		self.__gensparklinegraph(objects=objects, title='Sparkline for cumulative count on target', num_subplots=len(x.items()), perf_arr=perf_arr)

	def gentimesparklines(self):
		graph_names = ['Sparkline for cumulative time on origin', 'Sparkline for cumulative time on target']
		keys = (OrderedDict(sorted(self.cumulative[0].items(), key=operator.itemgetter(1), reverse=True)[:5])).keys()

		for p in 0,1:
			x = defaultdict(list)
			for (k, i), v in self.timesparklines[p].items():
				if k in keys:
					x[k].append(sum(v))

			x = OrderedDict(sorted(x.items(), key=lambda it: sum(it[1]), reverse=True))
			objects = self.__getrpc2name(x.keys())
			perf_arr = []

			for i in range(0, len(x.items())):
				k, v = (x.items())[i]
				perf_arr.append(v)
		
			self.__gensparklinegraph(objects=objects, title=graph_names[p], num_subplots=len(x.items()), perf_arr=perf_arr)
			
	def finalize(self):
		self.pp.close()
	
def main():
	print
	print
	print "*******************MARGO Profile Generator******************"
	print
	print "Reading CSV files from: " + os.getcwd()

	p = ProfileGenerator()
	p.readfiles()

	p.gencountgraph()
	p.gencountstatgraph()
	p.gencountrawgraph()

	p.gencumulativegraph()
	p.gencumulativestatgraph()
	p.gencumulativerawgraph()

	p.genargobotpoolsizegraph()

	p.gencountsparklines()
	p.gentimesparklines()

	p.finalize()

	print "Done."
	print
	print "************************************************************"


main()
